# (C) Copyright Confidential Containers Contributors 2023.
# SPDX-License-Identifier: Apache-2.0
#
# Run libvirt e2e tests.
name: (Callable) libvirt e2e tests

on:
  workflow_call:
    inputs:
      runner:
        default: 'ubuntu-24.04'
        description: The runner to execute the workflow on. Defaults to 'ubuntu-24.04'.
        required: false
        type: string
      podvm_image:
        required: true
        type: string
      caa_image:
        required: true
        type: string
      install_directory_artifact:
        description: The archive name of the install directory
        default: ''
        required: false
        type: string
      git_ref:
        default: 'main'
        description: Git ref to checkout the cloud-api-adaptor repository. Defaults to main.
        required: false
        type: string
      oras:
        description: Whether the podvm_image is oras published
        default: false
        required: false
        type: boolean
      container_runtime:
        default: 'containerd'
        description: Name of the container runtime. Either containerd or crio.
        required: false
        type: string
      install_method:
        default: 'kustomize'
        description: Installation method. Either kustomize or helm.
        required: false
        type: string
    secrets:
      REGISTRY_CREDENTIAL_ENCODED:
        required: true

env:
  CLOUD_PROVIDER: libvirt
  DEBIAN_FRONTEND: noninteractive

defaults:
  run:
    working-directory: src/cloud-api-adaptor

permissions: {}

jobs:
  test:
    runs-on: ${{ inputs.runner }}
    name: "Libvirt e2e tests: (${{ inputs.runner }}, ${{ inputs.container_runtime }}, mkosi: ${{ inputs.oras }}, ${{ inputs.install_method }}"
    steps:
      - name: Checkout Code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
        with:
          fetch-depth: 0
          ref: ${{ inputs.git_ref }}
          persist-credentials: false

      - name: Rebase the code
        if: github.event_name == 'pull_request_target'
        working-directory: ./
        run: |
          ./hack/ci-helper.sh rebase-atop-of-the-latest-target-branch

      - name: Remove unnecessary directories to free up space
        working-directory: ./
        run: |
          sudo ./hack/ci-helper.sh clean-up-runner

      - name: Read properties from versions.yaml
        run: |
          # yq may be pre-installed on self-hosted runners
          command -v yq || sudo snap install yq
          yq --version | grep -qE 'v4\.[0-9]+' || { echo "yq version is not 4.x, please install yq 4.x"; exit 1; }
          go_version="$(yq '.tools.golang' versions.yaml)"
          [ -n "$go_version" ]
          {
            echo "GO_VERSION=${go_version}"
            echo "ORAS_VERSION=$(yq -e '.tools.oras' versions.yaml)"
            echo "HELM_VERSION=$(yq -e '.tools.helm.version' versions.yaml)"
            echo "HELM_CHECKSUM=$(yq -e '.tools.helm.sha256' versions.yaml)"
          } >> "$GITHUB_ENV"

      - name: Setup Golang version ${{ env.GO_VERSION }}
        uses: actions/setup-go@4dc6199c7b1a012772edbd06daecab0f50c9053c # v6.1.0
        with:
          go-version: ${{ env.GO_VERSION }}
          cache-dependency-path: "**/go.sum"
          cache: false

      - name: Setup docker
        if: ${{ runner.environment == 'self-hosted' }}
        run: |
          sudo apt-get update -y
          sudo apt-get install -y docker.io
          sudo usermod -aG docker "$USER"

      - uses: oras-project/setup-oras@22ce207df3b08e061f537244349aac6ae1d214f6 # v1.2.4
        with:
          version: ${{ env.ORAS_VERSION }}

      - name: Extract qcow2 from ${{ inputs.podvm_image }}
        if: ${{ !inputs.oras }}
        run: |
           qcow2=$(echo ${{ inputs.podvm_image }} | sed -e "s#.*/\(.*\):.*#\1.qcow2#")
           ./hack/download-image.sh ${{ inputs.podvm_image }} . -o "${qcow2}" --clean-up
           echo "PODVM_QCOW2=$(pwd)/${qcow2}" >> "$GITHUB_ENV"
           # Clean up docker images to make space
           docker system prune -a -f
        working-directory: src/cloud-api-adaptor/podvm

      - name: Use oras to get qcow2 from ${{ inputs.podvm_image }}
        if: ${{ inputs.oras }}
        run: |
          oras pull ${{ inputs.podvm_image }}
          tar xvJpf podvm.tar.xz
          qcow2=$(find ./*.qcow2)
          echo "PODVM_QCOW2=$(pwd)/${qcow2}" >> "$GITHUB_ENV"
        working-directory: src/cloud-api-adaptor/podvm

      - name: Config Libvirt
        env:
          CONTAINER_RUNTIME: ${{ inputs.container_runtime }}
        run: |
          ./libvirt/config_libvirt.sh
          echo "container_runtime=\"${CONTAINER_RUNTIME}\"" >> libvirt.properties
          # For debugging
          cat libvirt.properties
          # Add the kcli install directory to PATH for later steps
          echo "${HOME}/.local/bin" >> "$GITHUB_PATH"

      - name: Install gh cli
        run: |
          sudo apt update -y
          sudo apt install -y gh

      - name: Double check that OVMF is installed
        run: |
          sudo apt update -y
          sudo apt install -y ovmf

      - name: Install kustomize
        if: ${{ inputs.install_method == 'kustomize' }}
        run: |
          command -v kustomize >/dev/null || \
          curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | \
            sudo bash -s /usr/local/bin

      - name: Install Helm
        if: ${{ inputs.install_method == 'helm' }}
        run: |
          curl -fsSL -o helm.tar.gz "https://get.helm.sh/helm-${HELM_VERSION}-linux-amd64.tar.gz"
          echo "${HELM_CHECKSUM}  helm.tar.gz" | sha256sum --check --strict
          tar -xzf helm.tar.gz
          sudo mv linux-amd64/helm /usr/local/bin/helm
          rm -rf helm.tar.gz linux-amd64
          helm version

# For the legacy packer approach we don't want to use the default firmware, so comment it out
      - name: Set blank firmware for packer libvirt tests
        if: ${{ inputs.install_method == 'kustomize' && !inputs.oras }}
        run: |
          cd "install/overlays/libvirt"
          sed -i 's/\(- LIBVIRT_EFI_FIRMWARE=.*\)/#\1/g' kustomization.yaml
          # Print for debugging
          echo "::group::Kustomization.yaml"
          cat kustomization.yaml
          echo "::endgroup::"

      - name: Update kustomization configuration
        if: ${{ inputs.install_method == 'kustomize' }}
        run: |
          cd "install/overlays/libvirt"
          kustomize edit set image "cloud-api-adaptor=${{ inputs.caa_image }}"
          # Print for debugging
          echo "::group::libvirt kustomization"
          cat kustomization.yaml
          echo "::endgroup::"

      - name: Checkout KBS Repository
        run: |
          test/utils/checkout_kbs.sh
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Provision cluster (helm only)
        if: ${{ inputs.install_method == 'helm' }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          CLOUD_PROVIDER: libvirt
          CONTAINER_RUNTIME: ${{ inputs.container_runtime }}
          TEST_PROVISION_FILE: ${{ github.workspace }}/src/cloud-api-adaptor/libvirt.properties
          TEST_PODVM_IMAGE: ${{ env.PODVM_QCOW2 }}
          TEST_E2E_TIMEOUT: "30m"
        run: make provision-cluster

      - name: Deploy CAA using Helm chart
        if: ${{ inputs.install_method == 'helm' }}
        env:
          CAA_IMAGE: ${{ inputs.caa_image }}
        run: |
          export KUBECONFIG="${HOME}/.kcli/clusters/peer-pods/auth/kubeconfig"

          # Install cert-manager (required for webhook)
          echo "Installing cert-manager..."
          make -C ../webhook deploy-cert-manager

          kubectl create namespace confidential-containers-system --dry-run=client -o yaml | kubectl apply -f -

          # Extract image name and tag from caa_image (format: registry/image:tag)
          IMAGE_FULL="${CAA_IMAGE}"
          IMAGE_TAG="${IMAGE_FULL##*:}"
          IMAGE_NAME="${IMAGE_FULL%:*}"

          echo "Deploying with Helm:"
          echo "  Image: ${IMAGE_NAME}"
          echo "  Tag: ${IMAGE_TAG}"

          # Build chart dependencies (downloads kata-deploy from OCI registry)
          echo "Building Helm chart dependencies..."
          helm dependency build ./install/charts/peerpods

          # TODO: Temporary solution. Providers lack Helm Apply() in test framework.
          #       Long term: implement HelmInstallOverlay.Apply() similar to other ProviderInstallOverlay implementations.
          # Read libvirt config from properties file (created by config_libvirt.sh)
          source libvirt.properties

          # Create SSH key secret for libvirt (keys created by config_libvirt.sh)
          # TODO: This should use --set-file providerSecrets.libvirt.id_rsa once
          #       providerSecrets.libvirt is defined in providers/libvirt.yaml
          kubectl create secret generic ssh-key-secret \
            --from-file=id_rsa="${HOME}/.ssh/id_rsa" \
            --from-file=id_rsa.pub="${HOME}/.ssh/id_rsa.pub" \
            -n confidential-containers-system

          # Install chart with libvirt configuration
          helm install peerpods ./install/charts/peerpods \
            -f install/charts/peerpods/providers/libvirt.yaml \
            --set "image.name=${IMAGE_NAME}" \
            --set "image.tag=${IMAGE_TAG}" \
            --set-string "providerConfigs.libvirt.LIBVIRT_URI=${libvirt_uri}" \
            -n confidential-containers-system \
            --wait --timeout=10m || {
              echo "::error::Helm install failed. Collecting debug info..."
              echo "::group::Pre-install job pods"
              kubectl get pods -n confidential-containers-system -l app.kubernetes.io/component=installer -o wide
              echo "::endgroup::"
              echo "::group::Pre-install job pod describe (shows OOMKilled, exit code, etc)"
              kubectl describe pods -n confidential-containers-system -l app.kubernetes.io/component=installer || true
              echo "::endgroup::"
              echo "::group::Pre-install job describe"
              kubectl describe job -n confidential-containers-system -l app.kubernetes.io/component=installer || true
              echo "::endgroup::"
              echo "::group::Pre-install job logs (current)"
              kubectl logs -n confidential-containers-system -l app.kubernetes.io/component=installer --tail=500 || true
              echo "::endgroup::"
              echo "::group::Pre-install job logs (previous - shows failed run)"
              kubectl logs -n confidential-containers-system -l app.kubernetes.io/component=installer --previous --tail=500 || true
              echo "::endgroup::"
              echo "::group::Pre-install job events"
              kubectl get events -n confidential-containers-system --sort-by='.lastTimestamp' | tail -50
              echo "::endgroup::"
              echo "::group::Webhook namespace check (peer-pods-webhook-system)"
              kubectl get namespace peer-pods-webhook-system 2>&1 || echo "Namespace does not exist"
              kubectl get events -n peer-pods-webhook-system --sort-by='.lastTimestamp' 2>&1 | tail -30 || true
              echo "::endgroup::"
              exit 1
            }

          echo "Waiting for CAA daemonset to be ready..."
          kubectl rollout status daemonset/cloud-api-adaptor-daemonset -n confidential-containers-system --timeout=5m

          echo "Waiting for kata-deploy daemonset to be ready..."
          kubectl rollout status daemonset/kata-deploy -n confidential-containers-system --timeout=5m

          # kata-deploy labels nodes with katacontainers.io/kata-runtime=true after installing kata.
          # The kata-remote RuntimeClass has a nodeSelector requiring this label.
          # We must wait for at least one node to have this label before tests can run.
          echo "Waiting for node to be labeled with katacontainers.io/kata-runtime=true..."
          timeout 120 bash -c '
            until kubectl get nodes -l katacontainers.io/kata-runtime=true --no-headers 2>/dev/null | grep -q .; do
              echo "  Waiting for kata-runtime node label..."
              sleep 5
            done
          '
          echo "Node labeled successfully:"
          kubectl get nodes -l katacontainers.io/kata-runtime=true

          echo "::group::Helm deployment info"
          helm list -n confidential-containers-system
          kubectl get daemonset -n confidential-containers-system
          kubectl get pods -n confidential-containers-system -l app=cloud-api-adaptor
          echo "::endgroup::"

      - name: run tests
        id: runTests
        env:
          AUTHENTICATED_REGISTRY_IMAGE: ${{ vars.AUTHENTICATED_REGISTRY_IMAGE }}
          REGISTRY_CREDENTIAL_ENCODED: ${{ secrets.REGISTRY_CREDENTIAL_ENCODED }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          INSTALL_METHOD: ${{ inputs.install_method }}
          CLOUD_PROVIDER: libvirt
          CONTAINER_RUNTIME: ${{ inputs.container_runtime }}
          DEPLOY_KBS: "true"
          TEST_TEARDOWN: "no"
          TEST_PROVISION_FILE: ${{ github.workspace }}/src/cloud-api-adaptor/libvirt.properties
          TEST_PODVM_IMAGE: ${{ env.PODVM_QCOW2 }}
          TEST_E2E_TIMEOUT: "75m"
        run: |
          # Default: provision cluster and install CAA
          export TEST_PROVISION="yes"
          export TEST_TEARDOWN="no"
          export TEST_PROVISION_FILE="$PWD/libvirt.properties"
          export TEST_PODVM_IMAGE="${{ env.PODVM_QCOW2 }}"
          export TEST_E2E_TIMEOUT="75m"

          # Skip provisioning and CAA installation if using helm (already done above)
          # KBS is deployed here (not in provision step) so keys match
          if [ "${INSTALL_METHOD}" = "helm" ]; then
            export TEST_PROVISION="no"
            export TEST_INSTALL_CAA="no"
            export KUBECONFIG="${HOME}/.kcli/clusters/peer-pods/auth/kubeconfig"
          fi

          make test-e2e

      - name: Debug tests failure
        if: failure() && steps.runTests.outcome == 'failure'
        timeout-minutes: 15
        working-directory: ./
        run: |
          export KUBECONFIG="${HOME}/.kcli/clusters/peer-pods/auth/kubeconfig"
          ./hack/ci-e2e-debug-fail.sh
        # Avoid running with `set -e` as command fails should be allowed
        shell: bash {0}

      - name: Clean-up cluster
        if: ${{ always() && runner.environment == 'self-hosted' }}
        run: ./libvirt/kcli_cluster.sh delete
